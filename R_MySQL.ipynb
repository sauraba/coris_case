{
    "metadata": {
        "language_info": {
            "name": "R", 
            "version": "3.3.2", 
            "mimetype": "text/x-r-source", 
            "codemirror_mode": "r", 
            "pygments_lexer": "r", 
            "file_extension": ".r"
        }, 
        "kernelspec": {
            "language": "R", 
            "name": "r-spark20", 
            "display_name": "R with Spark 2.0"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 11, 
            "cell_type": "code", 
            "source": "library(RMySQL)", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 12, 
            "cell_type": "code", 
            "source": "#Enter the values for you database connection\ndsn_database = \"ad_f21905e045c4fc2\"            \n            # e.g. \"db89c1c13a2014642a38dee00666f9d0c\"\ndsn_hostname = \"us-cdbr-iron-east-03.cleardb.net\" # e.g.: \"50.23.230.134\"\ndsn_port = 3306          # e.g. 3307 without quotes\ndsn_uid =  \"b32f3ab9a16720\"            # e.g. \"user1\"\ndsn_pwd = \"455fe273\"         # e.g. \"7dBZ3jWt9xN6$o0JiX!m\"", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 13, 
            "cell_type": "code", 
            "source": "conn = dbConnect(MySQL(), user=dsn_uid, password=dsn_pwd, dbname=dsn_database, host=dsn_hostname)\nconn", 
            "outputs": [
                {
                    "metadata": {}, 
                    "data": {
                        "text/plain": "<MySQLConnection:0,1>"
                    }, 
                    "output_type": "display_data"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 14, 
            "cell_type": "code", 
            "source": "query = \"select * from clustertable;\";\nrs = dbSendQuery(conn, query);\ndf = fetch(rs, -1);\nt<-df", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 9, 
            "cell_type": "code", 
            "source": "dbDisconnect(conn)", 
            "outputs": [
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "Warning message:\n\u201cClosing open result sets\u201d"
                }, 
                {
                    "metadata": {}, 
                    "data": {
                        "text/html": "TRUE", 
                        "text/latex": "TRUE", 
                        "text/plain": "[1] TRUE", 
                        "text/markdown": "TRUE"
                    }, 
                    "output_type": "display_data"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 9, 
            "cell_type": "code", 
            "source": "#load libraries \nlibrary(data.table)", 
            "outputs": [
                {
                    "traceback": [
                        "Error in library(h2o): there is no package called \u2018h2o\u2019\nTraceback:\n", 
                        "1. library(h2o)", 
                        "2. stop(txt, domain = NA)"
                    ], 
                    "evalue": "Error in library(h2o): there is no package called \u2018h2o\u2019\n", 
                    "output_type": "error", 
                    "ename": "ERROR"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "execution_count": 10, 
            "cell_type": "code", 
            "source": "setDT(df)", 
            "outputs": []
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 1, 
            "cell_type": "code", 
            "source": "dim(df)", 
            "outputs": [
                {
                    "metadata": {}, 
                    "data": {
                        "text/plain": "NULL"
                    }, 
                    "output_type": "display_data"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 12, 
            "cell_type": "code", 
            "source": "str(df)", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Classes \u2018data.table\u2019 and 'data.frame':\t8816 obs. of  3 variables:\n $ ASSETTAG : int  4689 4689 4689 4689 4689 4689 4689 4689 4689 4689 ...\n $ TIMESTAMP: int  1491398902 1491398927 1491398979 1491399030 1491399055 1491399082 1491399132 1491399183 1491399234 1491399286 ...\n $ CASETEMP : num  -84.6 -84.7 -85.1 -85.1 -85.5 ...\n - attr(*, \".internal.selfref\")=<externalptr> \n"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 24, 
            "cell_type": "code", 
            "source": "install.packages(\"flexdashboard\", type = \"source\")", 
            "outputs": [
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "Installing package into \u2018/gpfs/global_fs01/sym_shared/YPProdSpark/user/s2fc-27d9e54610dc0c-6f2ce6dcb959/R/libs\u2019\n(as \u2018lib\u2019 is unspecified)\nalso installing the dependencies \u2018backports\u2019, \u2018evaluate\u2019, \u2018highr\u2019, \u2018markdown\u2019, \u2018caTools\u2019, \u2018rprojroot\u2019, \u2018httpuv\u2019, \u2018xtable\u2019, \u2018sourcetools\u2019, \u2018htmltools\u2019, \u2018knitr\u2019, \u2018htmlwidgets\u2019, \u2018rmarkdown\u2019, \u2018shiny\u2019\n\nWarning message in install.packages(\"flexdashboard\", type = \"source\"):\n\u201cinstallation of package \u2018httpuv\u2019 had non-zero exit status\u201dWarning message in install.packages(\"flexdashboard\", type = \"source\"):\n\u201cinstallation of package \u2018shiny\u2019 had non-zero exit status\u201dWarning message in install.packages(\"flexdashboard\", type = \"source\"):\n\u201cinstallation of package \u2018flexdashboard\u2019 had non-zero exit status\u201d"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 15, 
            "cell_type": "code", 
            "source": "\n# Import Packages\nsuppressMessages(require(ibmdbR))\nsuppressMessages(require(lubridate))\nsuppressMessages(require(dtw))\nsuppressMessages(require(reshape2))\nsuppressMessages(require(optparse))\nsuppressMessages(require(jsonlite))\nsuppressMessages(require(xts))\n\n\n\n\n# Get time series data from a table\n# If an asset is specified, only get the data for that asset\ngetTimeSeriesData2 <-\n  function(\n    #dateToProcess,\n       #    dataStartDate,\n          # assets = NULL,\n           allAssetsDF) {\n    # Columns in the data table to capture\n    # cols_to_capture = 'ASSETTAG,TIMESTAMP,CASETEMP'\n    # if (is.null(assets)) {\n    #   # If assets parameter is null, return data for all cases after dataStartDate\n    #   query <- paste0('SELECT ',cols_to_capture,' FROM ',tableName,' WHERE TIMESTAMP > \\'',toString(dataStartDate),' 00:00:00\\'ORDER BY \"ASSETTAG\" ASC, \"TIMESTAMP\" ASC')\n    # } else {\n    #   # If assets parameter is not null, return data for that asset after dataStartDate\n    #   inStatement = paste0('ASSETTAG IN (\\'',paste0(assets,collapse='\\',\\''),'\\')')\n    #   query <- paste0('SELECT ',cols_to_capture,' FROM ',tableName,' WHERE ',inStatement,' AND TIMESTAMP > \\'',toString(dataStartDate),' 00:00:00\\'ORDER BY \"ASSETTAG\" ASC, \"TIMESTAMP\" ASC')\n    # }\n    # # Execute query\n    # allAssetsDF <- idaQuery(query)\n    \n    # Convert CASETEMP to numeric\n    allAssetsDF$CASETEMP <- as.numeric(allAssetsDF$CASETEMP)\n    # Impute instances when case temp is recorded as exactly -40 (why is this happening?)\n    for (i in which(allAssetsDF$CASETEMP == -40)) {\n      if (nrow(allAssetsDF) > i & i > 1) {\n        if (allAssetsDF[i - 1, \"ASSETTAG\"] == allAssetsDF[i, \"ASSETTAG\"] &\n            allAssetsDF[i + 1, \"ASSETTAG\"] == allAssetsDF[i, \"ASSETTAG\"]) {\n          allAssetsDF[i, \"CASETEMP\"] <-\n            mean(c(allAssetsDF[i - 1, \"CASETEMP\"], allAssetsDF[i + 1, \"CASETEMP\"]))\n        }\n      }\n    }\n    # Convert TIMESTAMP to datetime\n    allAssetsDF$TIMESTAMP <-\n      round(as.POSIXct(parse_date_time(allAssetsDF$TIMESTAMP, \"Ymd HMS\"), tz =\n                         \"UTC\"), units = \"mins\")\n    # Round TIMESTAMP to nearest 5 minutes\n    allAssetsDF$TIMESTAMP = format(\n      strptime(\"1970-01-01\", \"%Y-%m-%d\", tz = \"UTC\") +\n        round(as.numeric(allAssetsDF$TIMESTAMP) /\n                (5 * 60)) * (5 * 60),\n      \"%Y-%m-%d %H:%M:%S\"\n    )\n    # Make sure the datetime looks like: 2016-01-01 00:00:00\n    allAssetsDF$TIMESTAMP = as.POSIXct(as.character(allAssetsDF$TIMESTAMP), tz =\n                                         \"UTC\")\n    # Make a new column called DATE from the datetime\n    allAssetsDF$DATE = as.Date(allAssetsDF$TIMESTAMP, tz = \"UTC\")\n    # For each DATE, create a new column called DAYS_BACK,\n    #   which is 0 for the date that is being processed\n    #   and 1 for the day before the date to process, 2 for the next day, etc\n    #allAssetsDF$DAYS_BACK = as.integer(round(as.numeric(difftime(as.POSIXlt(dateToProcess,tz=\"UTC\"),as.POSIXlt(allAssetsDF$DATE,tz=\"UTC\"),tz=\"UTC\",units=\"days\"))))\n    return(allAssetsDF)\n  }\n\n\n\n\n\n\n\ngetDecomp2 <-\n  function(decompDataFilled,\n           freq = 288,\n           asset = \"UNKNOWN\") {\n    # Perform the time series decomposition\n    # Deal with the rare situation of a time series that starts or ends in NAs\n    \n    return (ts(decompDataFilled$CASETEMP, freq = freq))\n    \n    \n    \n  }\n\n\n\n\n# Given an asset, a data set of values from the STORE*** table, and the pivoted DTW Table,\n# calculate the case health scores for that one asset\n# date = date to process the scores for\n# store = store number\n# asset = ASSETTAG\n# assetDF = a dataframe containing the data (ASSETTAG, TIMESTAMP, CASETEMP) for this one asset for\n#   last few weeks (default = 21 days)\n# assetDTWDF = a pivoted assetDF, as described by the function: createDTWtable()\n# maxNAprop = if the proportion of missing values in the Date To Process is greater than this value,\n#   return NAs for the scores\n# hoursInPeriod: how many hours in one period for this case. Default is 24\nprocessAsset2 = function(date,\n                         asset,\n                         assetDF,\n                         maxNAprop = 0.1,\n                         hoursInPeriod = 24) {\n  \n  # Remove any duplicate readings for this asset. Shouldn't be any\n  assetDF <- assetDF[!duplicated(assetDF$TIMESTAMP),]\n  # In the next few lines, I'm going to try to automatically determine the data frequency\n  # Take 2 contingious samples (I used the 2nd and 3rd reading) below\n  # Calculate the time difference between them in minutes (sampleIntervalMin)\n  # Figure out how many of readings of sampleIntervalMin fit into one period (default = 24 hours)\n  # ^ that is the value of \"frequency\" or \"freq\" that will be passed to time series decomposition\n  # NOTE: There is a small danger here... the timediff between readings 2 and 3 might have a gap\n  #     which throws off the calculation of how many readings per day there are.\n  sampleIntervalMin = as.numeric(difftime(assetDF$TIMESTAMP[3], assetDF$TIMESTAMP[2], units =\n                                            \"mins\"))\n  samplesIn1Period = (hoursInPeriod * 60) / sampleIntervalMin\n  # Compile data for decomp into the dataframe: decompData. Order by TIMESTAMP\n  decompData = assetDF[order(assetDF$TIMESTAMP), c(\"TIMESTAMP\", \"CASETEMP\")]\n  # There might be missing values in the data. \"full\" is a sequence from the start of data until the end,\n  #    using whatever timediff was present between readings 2 and 3 (arbitrarily selected contingious points)\n  # I'm creating a new dataframe that has a row for EVERY point in time that there SHOULD be data.\n  #  Then I'm merging the ACTUAL data into that. That way, any missing values in the time series.\n  #  That way, we won't have any gaps. They'll be filled with NAs\n  full = seq(\n    from = min(assetDF$TIMESTAMP),\n    to = max(assetDF$TIMESTAMP),\n    by = difftime(assetDF$TIMESTAMP[3], assetDF$TIMESTAMP[2], units = \"mins\")\n  )\n  decompDataFilled = data.frame(TIMESTAMP = full)\n  decompDataFilled = merge(decompDataFilled,\n                           decompData[, c(\"TIMESTAMP\", \"CASETEMP\")],\n                           by = \"TIMESTAMP\",\n                           all.x = T)\n  decompDataFilled = decompDataFilled[complete.cases(decompDataFilled$TIMESTAMP),]\n  # Make sure everything is still ordered by TIMESTAMP\n  decompDataFilled = decompDataFilled[order(decompDataFilled$TIMESTAMP), c(\"TIMESTAMP\", \"CASETEMP\")]\n  # Only keep data for days that are on or before the dateToProcess\n  #decompDataFilled = decompDataFilled[decompDataFilled$DAYS_BACK >= 0,]\n  # Deal with the rare situation of a timeseries that stars or ends with NAs\n  while (is.na(decompDataFilled[1, \"CASETEMP\"])) {\n    decompDataFilled = decompDataFilled[2:nrow(decompDataFilled),]\n  }\n  while (is.na(decompDataFilled[nrow(decompDataFilled), \"CASETEMP\"])) {\n    decompDataFilled = decompDataFilled[1:nrow(decompDataFilled) - 1,]\n  }\n  # Calculate the time series decomposition scores.\n  x <-\n    getDecomp2(decompDataFilled, freq = samplesIn1Period, asset = asset)\n  \n  # Pull out the time series for the date to process\n  # and the time series for the historical median.\n  # Then, calculate the DTW distance\n  # ts1 <- assetDTWDF[,c(\"0\")]\n  # ts2 <- assetDTWDF[,c(\"HIST_MEDIAN\")]\n  \n  # Compile the scores into a new row, which will get added to the health report.\n  # assetValues <- data.frame(\n  #   ASSET=asset,\n  #   STORE=asset,\n  #   LASTFULLDAY=date,\n  #   TREND1=trend1,\n  #   TREND3=trend3,\n  #   TREND5=trend5,\n  #   RANDOM=volscore,\n  #   ANOMALY=anomaly,\n  #   NUMNA=propNA\n  # )\n  return(x)\n}\n\n\n\n\n\n\n# This is the primary function\n# Given a store or list of stores, a start date, and an end date,\n#    it will process all the assets in that store (or stores) for the given days\n# A list of ASSETTAGs can also be supplied, in which case only those assets will have scores calculated\n#    Note: if assetsToProcess is defined, you still need to provide the corresponding store numbers\n# host / user / password / dbName: connection details to database\n#    If a host is not specified, assume this is running locally within DashDB\n# tablePrefix: default is \"STORE\". We assume the data table for store 919 is: tablePrefix+StoreName = STORE919\n# decompLastNDays: How many days worth of data to pull for decomposition. 21 seems to work well.\n#    More data means the STL decomp function will see more periods, but it also will take longer to pull data\n# getAssetsLastNDays: If no assets are provided, the script gets a list of unique assets in the store table.\n#    This function determines how many days back to look for unique asset names\n# hoursInPeriod: how many hours in one full period of case temperature data (to capture seasonality). 24 hours is default\ngetCentroidDTW = function(# stores,\n  dateStart = NULL,\n  dateEnd = NULL,\n  assetsToProcess = NULL,\n  decompLastNdays = 21,\n  getAssetsLastNdays = 3,\n  hoursInPeriod = 24,\n  t,\n  currentTime) {\n  \n  \n  # Initialize the case health report\n  health_rep <- data.frame(\n    ASSET = as.character(),\n    STORE = as.character(),\n    LASTFULLDAY = as.Date(as.character()),\n    TREND1 = as.numeric(),\n    TREND3 = as.numeric(),\n    TREND5 = as.numeric(),\n    RANDOM = as.numeric(),\n    ANOMALY = as.numeric(),\n    NUMNA = as.integer()\n  )\n  # dateToProcess<- dates\n  # dateToProcess = as.Date(dateToProcess, origin = \"1970-01-01\", tz =\n  #                           \"UTC\")\n  \n  # allAssetsDF <- t[which(as.POSIXlt(dateEnd+1,origin=\"1970-01-01\", \"UTC\")-t$TIMESTAMP>0),]\n  # allAssetsDF<-t[which(t$TIMESTAMP-as.POSIXlt(dateStart-21,origin=\"1970-01-01\", \"UTC\")>0),]\n  allAssetsDF <- t\n  allAssetsDF <-\n    getTimeSeriesData2(\n      #dateToProcess = dateStartDate,\n     # assets = assets,\n     # dataStartDate = dateStartDate,\n      allAssetsDF\n    )\n  \n  distMatrix<- data.frame()\n  score<-data.frame()\n  # Pivot the asset data as described in the createDTWtable.\n  # The primary key for each row is an ASSETTAG and a time of day (examples: 00:00, 00:05, etc)\n  # The column \"0\" contains the time series for that ASSETTAG at the time of day on the date to process\n  # The column \"1\" contains the time series for that ASSETTAG at the time of day for N-1 days\n  # The column \"2\" contains the time series for that ASSETTAG at the time of day for N-2 days\n  # allDtwDF <- createDTWtable(allAssetsDF,dtwUseLastNDays=decompLastNdays)\n  \n  dfTimeSeries <- data.frame()\n  # Now that the data tables have been all setup and we have a list of assets...\n  # calculate the case health scores for each of the assets in this store\n  #for (asset in assets) {\n  assetList <- unique(t$ASSETTAG)\n  i <- 1\n  \n  \n distMatrix<- matrix(1:100,10,10)\n  for (i in 1:10)\n  {\n    asset1 <- assetList[i]\n    for (j in 1:10)\n    {\n      if (i != j)\n      {\n        asset2 <- assetList[j]\n        \n        # Just get the subset of the store data for this asset\n        assetDF1 <-\n          allAssetsDF[allAssetsDF$ASSETTAG == asset1, c(\"TIMESTAMP\", \"CASETEMP\")]\n        tsAsset1 <-\n          processAsset2(\n            date = dateToProcess,\n            asset1,\n            assetDF1,\n            maxNAprop = 0.5,\n            hoursInPeriod = hoursInPeriod\n          )\n        assetDF2 <-\n          allAssetsDF[allAssetsDF$ASSETTAG == asset2, c(\"TIMESTAMP\", \"CASETEMP\")]\n        tsAsset2 <-\n          processAsset2(\n            date = dateToProcess,\n            asset2,\n            assetDF2,\n            maxNAprop = 0.5,\n            hoursInPeriod = hoursInPeriod\n          )\n        dfTimeSeries <- t(as.data.frame(tsAsset1))\n        dfTimeSeries2 <- t(as.data.frame(tsAsset2))\n        dfTimeSeries[,which(is.na(dfTimeSeries))]<- mean(assetDF1$CASETEMP)\n        dfTimeSeries2[,which(is.na(dfTimeSeries2))]<- mean(assetDF2$CASETEMP)\n        if(ncol(dfTimeSeries)<ncol(dfTimeSeries2))\n        {dfTimeSeries3 <-\n          rbind(dfTimeSeries, dfTimeSeries2[, 1:ncol(dfTimeSeries)])}\n        else\n        {dfTimeSeries3 <-\n          rbind(dfTimeSeries[, 1:ncol(dfTimeSeries2)], dfTimeSeries2)}\n        distMatrix[i, j] <- dist(dfTimeSeries3, method = 'DTW')\n        \n      }\n    }\n    \n  }\n  \n  return(distMatrix)\n}\n\ngetClusterDTW = function(# stores,\n  dateStart = NULL,\n  dateEnd = NULL,\n  assetsToProcess = NULL,\n  decompLastNdays = 21,\n  getAssetsLastNdays = 3,\n  hoursInPeriod = 24,\n  t,\n  currentTime) {\n  # Determine what day it is today, and when the last full day's worth of data is (yesterday)\n  \n  # currentDate = as.Date(currentTime,origin=\"1970-01-01\",tz=\"UTC\")\n  # lastFullDay = as.Date(currentDate - 1,origin=\"1970-01-01\",tz=\"UTC\")\n  #\n  # # If you didn't specify a day to start on, assume you want yesterday's scores\n  # dateStart = ifelse(is.null(dateStart),lastFullDay,dateStart)\n  # #dateStart<-lastFullDay\n  # dateStart = as.Date(dateStart,origin=\"1970-01-01\",tz=\"UTC\")\n  # #dateEnd=\"2015-06-01T00:00:18Z UTC\"\n  # # If you didn't specify a day to end on, assume you only want dateStart's scores\n  # dateEnd = lastFullDay\n  # dateEnd = as.Date(dateEnd,origin=\"1970-01-01\",tz=\"UTC\")\n  # Create a sequence of dates between dateStart and dateEnd\n  datesToProcess = as.Date(seq.Date(dateStart, dateEnd, by = \"days\"), tz =\n                             \"UTC\")\n  \n  dfTimeSeries3<-data.frame()\n  # Initialize the case health report\n  health_rep <- data.frame(\n    ASSET = as.character(),\n    STORE = as.character(),\n    LASTFULLDAY = as.Date(as.character()),\n    TREND1 = as.numeric(),\n    TREND3 = as.numeric(),\n    TREND5 = as.numeric(),\n    RANDOM = as.numeric(),\n    ANOMALY = as.numeric(),\n    NUMNA = as.integer()\n  )\n  \n  dateToProcess = as.Date(dateToProcess, origin = \"1970-01-01\", tz =\n                            \"UTC\")\n  \n  # allAssetsDF <- t[which(as.POSIXlt(dateEnd+1,origin=\"1970-01-01\", \"UTC\")-t$TIMESTAMP>0),]\n  # allAssetsDF<-t[which(t$TIMESTAMP-as.POSIXlt(dateStart-21,origin=\"1970-01-01\", \"UTC\")>0),]\n  allAssetsDF <- t\n  allAssetsDF <-\n    getTimeSeriesData2(\n      #dateToProcess = dateToProcess,\n      #assets = assets,\n      #dataStartDate = dateStartDate,\n      allAssetsDF\n    )\n  # Pivot the asset data as described in the createDTWtable.\n  # The primary key for each row is an ASSETTAG and a time of day (examples: 00:00, 00:05, etc)\n  # The column \"0\" contains the time series for that ASSETTAG at the time of day on the date to process\n  # The column \"1\" contains the time series for that ASSETTAG at the time of day for N-1 days\n  # The column \"2\" contains the time series for that ASSETTAG at the time of day for N-2 days\n  # allDtwDF <- createDTWtable(allAssetsDF,dtwUseLastNDays=decompLastNdays)\n  \n  dfTimeSeries3 <- data.frame()\n  # Now that the data tables have been all setup and we have a list of assets...\n  # calculate the case health scores for each of the assets in this store\n  #for (asset in assets) {\n  assetList <- unique(t$ASSETTAG)\n  i <- 1\n  for (i in 1:10)\n  {\n    asset <- assetList[i]\n    \n    \n    # Just get the subset of the store data for this asset\n    assetDF <-\n      allAssetsDF[allAssetsDF$ASSETTAG == asset, c(\"TIMESTAMP\", \"CASETEMP\")]\n    tsAsset <-\n      processAsset2(\n        date = dateToProcess,\n        asset,\n        assetDF,\n        maxNAprop = 0.5,\n        hoursInPeriod = hoursInPeriod\n      )\n    \n    dfTimeSeries <- t(as.data.frame(tsAsset))\n    \n  \n    \n    if((i==1))\n    {dfTimeSeries3 <- dfTimeSeries\n    } else{\n      if(ncol(dfTimeSeries)<ncol(dfTimeSeries3))\n      {dfTimeSeries3 <-\n        rbind(dfTimeSeries3, dfTimeSeries[, 1:ncol(dfTimeSeries)])}\n      else\n      {dfTimeSeries3 <-\n        rbind(dfTimeSeries3,  dfTimeSeries[, 1:ncol(dfTimeSeries3)])}\n      \n  \n      }\n    \n  #   else\n  #   {dfTimeSeries3 <-\n  #     rbind(dfTimeSeries3, dfTimeSeries[, 1:ncol(dfTimeSeries)])}\n  #   \n  }\n  return(dfTimeSeries3)\n  # distMatrix_all <- dist(dfTimeSeries3, method = 'DTW')\n  # hc <- hclust(distMatrix_all, method = 'average')\n  # plot(hc, labels = observedLabels, main = '')\n  # \n}\n\nwriteHealthRepToFile = function(health_rep,\n                                filenamePrefix = \"health_report_001_\",\n                                fileNameSuffix = \"_scaledVolScore.csv\",\n                                dateStart = \"unknownStartDate\",\n                                dateEnd = \"unknownEndDate\")\n{\n  fileNameToWrite = paste0(filenamePrefix, dateStart, \"_to_\", dateEnd, fileNameSuffix)\n  warning(paste0(\"Current Directory: \", getwd()))\n  warning(paste0(\"Writing to file: \", fileNameToWrite))\n  write.table(\n    health_rep,\n    fileNameToWrite,\n    row.names = F,\n    quote = F,\n    sep = \",\"\n  )\n}\n\n\n# Given a health report, write the table to the database\n# health_rep is the health report data frame\nwriteHealthRepToDB = function(health_rep,\n                              host = NULL,\n                              user = \"bluadmin\",\n                              password = \"ZmUzYzFhOWMzODI3\",\n                              dbName = \"BLUDB\",\n                              tempTable = \"CASEHEALTH_TMP\",\n                              caseHealthTable = \"CASEHEALTH\") {\n  health_rep$ASSET = as.character(health_rep$ASSET)\n  health_rep = health_rep[!duplicated(health_rep),]\n  \n  warning(\n    paste0(\n      \"Attempting to write entire health report to casehealth temp table: \",\n      tempTable\n    )\n  )\n  # Save the entire health report dataframe to CASEHEALTH_TMP in dashDB (overwrite whatever is there)\n  idf = tryCatch({\n    idf <- as.ida.data.frame(health_rep, tempTable, clear.existing = T)\n  }, warning = function(w) {\n    warning(\"Warning while trying to upload Case Health Report as CASEHEALTH_TMP\")\n    warning(w)\n  }, error = function(e) {\n    warning(\"Error while trying to upload Case Health Report as CASEHEALTH_TMP\")\n    warning(e)\n  })\n  \n  # Go through each row in the new health report.\n  # First, try sqlSave. That may fail if the key (ASSET, LASTFULLDAY) already exists\n  # Next, try sqlUpdate. Should overwrite the existing value if the primary key already exists\n  warning(\n    paste0(\n      \"Attempting to write individual rows of data to Case Health table: \",\n      caseHealthTable\n    )\n  )\n  for (i in 1:nrow(health_rep)) {\n    result = tryCatch({\n      sqlSave(\n        channel = con,\n        dat = health_rep[i,],\n        tablename = caseHealthTable,\n        verbose = F,\n        append = T,\n        rownames = F,\n        colnames = F\n      )\n    }, warning = function(w) {\n      warning(\"Warning while trying to sqlSave row to table\")\n      warning(w)\n    }, error = function(e) {\n      warning(\"Error while trying to sqlSave row to table. Trying sqlUpdate instead\")\n      warning(e)\n      tryCatch({\n        sqlUpdate(\n          channel = con,\n          dat = health_rep[i,],\n          tablename = caseHealthTable,\n          verbose = F,\n          index = c(\"ASSET\", \"LASTFULLDAY\")\n        )\n      }, warning = function(w) {\n        warning(\"Warning while trying to sqlUpdate row to table\")\n        warning(w)\n      }, error = function(e) {\n        warning(\"Error while trying to sqlUpdate row to table\")\n        warning(e)\n      })\n    })\n  }\n  \n  return(health_rep)\n  \n}\n\n\n\n\n\n\n\n\n\n\n################### RUN THE CASE HEALTH  ###############################\n\n\nt <- read.csv(\"21days - 10 Sensors.csv\")\nt$TIMESTAMP <- as.POSIXlt(t$TIMESTAMP, origin = \"1970-01-01\", \"UTC\")\ncurrentDate = as.Date(max(t$TIMESTAMP, na.rm = TRUE),\n                      origin = \"1970-01-01\",\n                      tz = \"UTC\")\ncurrentTime = as.POSIXlt(max(t$TIMESTAMP, na.rm = TRUE), origin = \"1970-01-01\", \"UTC\")\ndateStart <-\n  as.Date(currentDate - 21, origin = \"1970-01-01\", tz = \"UTC\")\ndateEnd <-\n  as.Date(currentDate , origin = \"1970-01-01\", tz = \"UTC\")\ndateToProcess<- dateStart\n\n\n\n\n\nhealth_rep_centroid = getCentroidDTW(\n  # stores=stores,\n  dateStart = dateStart,\n  dateEnd = dateEnd,\n  assetsToProcess = NULL,\n  decompLastNdays = 21,\n  getAssetsLastNdays = 3,\n  hoursInPeriod = 24,\n  t,\n  currentTime\n)\nwrite.csv(file = \"health_rep_centroid.csv\", health_rep_centroid)\n\nhealth_rep_cluster = getClusterDTW(\n  # stores=stores,\n  dateStart = dateStart,\n  dateEnd = dateEnd,\n  assetsToProcess = NULL,\n  decompLastNdays = 21,\n  getAssetsLastNdays = 3,\n  hoursInPeriod = 24, \n  t,\n  currentTime\n) \n", 
            "outputs": [
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n\u201cthere is no package called \u2018lubridate\u2019\u201dWarning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n\u201cthere is no package called \u2018dtw\u2019\u201dWarning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n\u201cthere is no package called \u2018optparse\u2019\u201dWarning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n\u201cthere is no package called \u2018xts\u2019\u201dWarning message in file(file, \"rt\"):\n\u201ccannot open file '21days - 10 Sensors.csv': No such file or directory\u201d"
                }, 
                {
                    "traceback": [
                        "Error in file(file, \"rt\"): cannot open the connection\nTraceback:\n", 
                        "1. read.csv(\"21days - 10 Sensors.csv\")", 
                        "2. read.table(file = file, header = header, sep = sep, quote = quote, \n .     dec = dec, fill = fill, comment.char = comment.char, ...)", 
                        "3. file(file, \"rt\")"
                    ], 
                    "evalue": "Error in file(file, \"rt\"): cannot open the connection\n", 
                    "output_type": "error", 
                    "ename": "ERROR"
                }
            ]
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "execution_count": 17, 
            "cell_type": "code", 
            "source": "\ninstall.packages(lubridate)\ninstall.packages(dtw)\ninstall.packages(reshape2)\ninstall.packages(optparse)\ninstall.packages(jsonlite)\ninstall.packages(xts)\n", 
            "outputs": [
                {
                    "traceback": [
                        "Error in install.packages(lubridate): object 'lubridate' not found\nTraceback:\n", 
                        "1. install.packages(lubridate)"
                    ], 
                    "evalue": "Error in install.packages(lubridate): object 'lubridate' not found\n", 
                    "output_type": "error", 
                    "ename": "ERROR"
                }
            ]
        }
    ], 
    "nbformat_minor": 0
}